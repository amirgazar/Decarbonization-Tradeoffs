total_PM2.5_mUSD      = mean(total_PM2.5_USD, na.rm = TRUE)/1e6,
total_PM10_mUSD      = mean(total_PM10_USD, na.rm = TRUE)/1e6,
total_NOx_mUSD       = mean(total_NOx_USD, na.rm = TRUE)/1e6,
total_SO2_mUSD       = mean(total_SO2_USD, na.rm = TRUE)/1e6,
total_VOC_mUSD      = mean(total_VOC_USD, na.rm = TRUE)/1e6,
total_CO_mUSD      = mean(total_CO_USD, na.rm = TRUE)/1e6,
total_AP_mUSD = sum(total_PM2.5_mUSD, total_PM10_mUSD, total_NOx_mUSD, total_SO2_mUSD,
total_VOC_mUSD, total_CO_mUSD),
AP_death_per_year = total_AP_mUSD/mortality_million_USD_2024,
.groups = "drop"
)
death_data <- mean_simulations %>%
select(Facility_Unit.ID, Year, AP_death_per_year) %>%
group_by(Facility_Unit.ID) %>%
summarise(total_AP_death = sum(AP_death_per_year, na.rm = TRUE), .groups = "drop")
# Merge with subset_data, adding only the death column
merged_data <- subset_data %>%
inner_join(death_data, by = "Facility_Unit.ID")
# adjust the last row (to ensure the exact amount of cumsum is achieved)
factor <- (merged_data$cum_mean[merged_data$Facility_Unit.ID == "6156_NHB1"] - new_capacity) /
merged_data$mean_gen_MW[merged_data$Facility_Unit.ID == "6156_NHB1"]
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] <-
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] * factor
final_data <- merged_data %>%
select(State, total_AP_death) %>%
group_by(State) %>%
summarise(total_AP_death_state = sum(total_AP_death, na.rm = TRUE), .groups = "drop")
# Facility Capacity Data
subset_data$Estimated_NameplateCapacity_MW[subset_data$Facility_Unit.ID == "6156_NHB1"] <-
subset_data$Estimated_NameplateCapacity_MW[subset_data$Facility_Unit.ID == "6156_NHB1"] * factor
subset_data$Real_NameplateCapacity_MW <- subset_data$Estimated_NameplateCapacity_MW * subset_data$CF
facility_data <- subset_data %>%
select(Unit_Type, Real_NameplateCapacity_MW) %>%
group_by(Unit_Type) %>%
summarise(total_NPC = sum(Real_NameplateCapacity_MW, na.rm = TRUE), .groups = "drop")
View(facility_data)
463.28426+70.37462
# Dispatch curve, offsetting demand with generation
library(data.table)
library(httr)
library(htmltools)
library(jsonlite)
library(lubridate)
calculate_npv <- function(dt, rate, base_year, col) {
npv <- sum(dt[[col]] / (1 + rate)^(dt[["Year"]] - base_year))
return(npv)
}
discount_rate <- 0.03
base_year <- 2024
# Cost of mortality: $11.4 million in 2024-USD per avoided mortality
mortality_million_USD_2024 <- 11.4
# 1.1 Fossil Fuels Data
file_path <- "/Users/amirgazar/Documents/GitHub/EPA_Debarbonization/2 Generation Expansion Model/2 Generation/2 Fossil Generation/1 Existing Fossil Fuels/1 Fossil Fuels Facilities Data/Fossil_Fuel_Facilities_Data.csv"
Fossil_Fuels_NPC <- fread(file_path)
Fossil_Fuels_NPC <- Fossil_Fuels_NPC[Retirement_year >= 2025]
# Order data by CF (smallest to largest)
Fossil_Fuels_NPC_sorted <- Fossil_Fuels_NPC[order(Fossil_Fuels_NPC$CF), ]
# Compute cumulative sum of mean_gen_MW
Fossil_Fuels_NPC_sorted$cum_mean <- cumsum(Fossil_Fuels_NPC_sorted$mean_gen_MW)
# Find the index where the cumulative sum reaches or exceeds the new project's capacity
new_capacity <- 900 # Transmission scenario
new_capacity <- 541.63032 # Offshore wind scenario
idx <- which(Fossil_Fuels_NPC_sorted$cum_mean >= new_capacity)[1]
# Subset the rows that contribute to the cumulative sum
subset_data <- Fossil_Fuels_NPC_sorted[1:idx, ]
# RUN 10_Air_emissions.R CODE FIRST to generate Facility_Level_Results
filtered_data <- Facility_Level_Results %>%
filter(Pathway == "A",
Facility_Unit.ID %in% subset_data$Facility_Unit.ID)
mean_simulations <- filtered_data %>%
group_by(Facility_Unit.ID, Year) %>%
summarise(
Year                = first(Year),         # constant across sims
total_generation_GWh= mean(total_generation_GWh, na.rm = TRUE),
total_PM2.5_mUSD      = mean(total_PM2.5_USD, na.rm = TRUE)/1e6,
total_PM10_mUSD      = mean(total_PM10_USD, na.rm = TRUE)/1e6,
total_NOx_mUSD       = mean(total_NOx_USD, na.rm = TRUE)/1e6,
total_SO2_mUSD       = mean(total_SO2_USD, na.rm = TRUE)/1e6,
total_VOC_mUSD      = mean(total_VOC_USD, na.rm = TRUE)/1e6,
total_CO_mUSD      = mean(total_CO_USD, na.rm = TRUE)/1e6,
total_AP_mUSD = sum(total_PM2.5_mUSD, total_PM10_mUSD, total_NOx_mUSD, total_SO2_mUSD,
total_VOC_mUSD, total_CO_mUSD),
AP_death_per_year = total_AP_mUSD/mortality_million_USD_2024,
.groups = "drop"
)
death_data <- mean_simulations %>%
select(Facility_Unit.ID, Year, AP_death_per_year) %>%
group_by(Facility_Unit.ID) %>%
summarise(total_AP_death = sum(AP_death_per_year, na.rm = TRUE), .groups = "drop")
# Merge with subset_data, adding only the death column
merged_data <- subset_data %>%
inner_join(death_data, by = "Facility_Unit.ID")
# adjust the last row (to ensure the exact amount of cumsum is achieved)
factor <- (merged_data$cum_mean[merged_data$Facility_Unit.ID == "6156_NHB1"] - new_capacity) /
merged_data$mean_gen_MW[merged_data$Facility_Unit.ID == "6156_NHB1"]
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] <-
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] * factor
final_data <- merged_data %>%
select(State, total_AP_death) %>%
group_by(State) %>%
summarise(total_AP_death_state = sum(total_AP_death, na.rm = TRUE), .groups = "drop")
# Facility Capacity Data
subset_data$Estimated_NameplateCapacity_MW[subset_data$Facility_Unit.ID == "6156_NHB1"] <-
subset_data$Estimated_NameplateCapacity_MW[subset_data$Facility_Unit.ID == "6156_NHB1"] * factor
subset_data$Real_NameplateCapacity_MW <- subset_data$Estimated_NameplateCapacity_MW * subset_data$CF
facility_data <- subset_data %>%
select(Unit_Type, Real_NameplateCapacity_MW) %>%
group_by(Unit_Type) %>%
summarise(total_NPC = sum(Real_NameplateCapacity_MW, na.rm = TRUE), .groups = "drop")
View(facility_data)
399.32747+70.37462
# Load necessary library
library(dplyr)
library(data.table)
# Define state codes and base file path
state_codes <- c("NH", "MA")
base_path <- "/Users/amirgazar/Documents/GitHub/WTP Survey/openaddresses datasets/"
# Initialize an empty list to store each state's address data
address_list <- list()
# Loop through each state and read the corresponding address file
for (state in state_codes) {
# Construct the file path for each state's addresses file
file_path <- paste0(base_path, state, "/random_addresses_", state, ".csv")
# Read the address file and add it to the list if it exists
if (file.exists(file_path)) {
state_data <- read.csv(
file_path,
stringsAsFactors = FALSE,
na.strings = c("NA", "NaN", "NAN")  # Treat "NaN" and "NAN" as NA
)
set.seed(42)  # Optional: Set seed for reproducibility
# Generate the recipient_id based on the rule
state_data <- state_data %>%
mutate(
control_number = sample(0:9, n(), replace = TRUE),  # Random control number (0-9) for each row
control_letter = sample(LETTERS, n(), replace = TRUE),  # Random control letter (A-Z) for each row
recipient_id = paste0(
toupper(substr(address_city, nchar(address_city) - 1, nchar(address_city))), # Last two letters of city
substr(sprintf("%05s", address_ZIPPlus4), 2, 3), # Middle two digits of ZIP code
control_number, control_letter
),
unit = ifelse(is.na(address_secondaryAddress) | address_secondaryAddress == "",
"", address_secondaryAddress),
street = ifelse(is.na(address_streetAddress) | address_streetAddress == "",
"", address_streetAddress),
city = ifelse(is.na(address_city) | address_city == "",
"", address_city),
state = ifelse(is.na(address_state) | address_state == "",
"", address_state),
zip = ifelse(is.na(address_ZIPPlus4) | address_ZIPPlus4 == "",
sprintf("%05s", address_ZIPCode), # Only 5-digit ZIP if ZIP+4 is missing
paste0(sprintf("%05s", address_ZIPCode), "-", address_ZIPPlus4)
)
) %>%
select(recipient_id, unit, street, city, state, zip) %>%
# Replace any remaining NA values with empty strings
mutate(across(everything(), ~ ifelse(is.na(.), "", .)))
# Append to the list
address_list[[state]] <- state_data
} else {
warning(paste("File not found for state:", state))
}
}
# Combine all state data frames into a single data frame
addresses <- bind_rows(address_list)  # Using bind_rows from dplyr for better handling
# ============================================
# Step 1: Identify and Display Duplicate Addresses
# ============================================
# Define the columns that determine a unique address
# Adjust these columns based on your data structure
unique_address_columns <- c("unit", "street", "city", "state", "zip")
# Identify duplicates
duplicates <- addresses %>%
group_by(across(all_of(unique_address_columns))) %>%
filter(n() > 1) %>%
ungroup()
# Check if there are any duplicates
if(nrow(duplicates) > 0){
cat("Duplicate Addresses Found:\n")
print(duplicates)
# Optionally, you can save duplicates to a separate CSV for review
duplicates_path <- "/Users/amirgazar/Documents/GitHub/WTP Survey/openaddresses datasets/Final/duplicate_addresses.csv"
write.csv(duplicates, duplicates_path, row.names = FALSE, na = "")
# ============================================
# Step 2: Remove Duplicate Addresses
# ============================================
# Remove duplicates, keeping the first occurrence
addresses_unique <- addresses %>%
distinct(across(all_of(unique_address_columns)), .keep_all = TRUE)
cat("\nDuplicates have been removed. Number of unique addresses:", nrow(addresses_unique), "\n")
} else {
cat("No duplicate addresses found.\n")
addresses_unique <- addresses
}
# ============================================
# Step 3: Save the Cleaned Data to CSV
# ============================================
# Define the final output file path
final_path <- "/Users/amirgazar/Documents/GitHub/WTP Survey/openaddresses datasets/Final/combined_addresses.csv"
# Save the combined (and deduplicated) addresses to a CSV file in the Final folder
write.csv(addresses_unique, final_path, row.names = FALSE, na = "")  # Write NA as empty strings
cat("Combined addresses have been saved to:", final_path, "\n")
# Define the folder path
folder_path <- "/Users/amirgazar/Documents/GitHub/WTP Survey/openaddresses datasets/Final/"
# Get the list of batch folders
batch_folders <- list.dirs(folder_path, recursive = FALSE)
# Read and merge all batch files dynamically
address_list <- lapply(batch_folders, function(batch) {
file_path <- file.path(batch, "combined_addresses.csv")
if (file.exists(file_path)) {
fread(file_path)
} else {
NULL
}
})
# Remove NULL entries (in case any file is missing)
address_list <- Filter(Negate(is.null), address_list)
# Combine all datasets into one
merged_addresses <- bind_rows(address_list)
address_columns <- c("unit", "street", "city", "state", "zip")
# Create a logical vector indicating duplicated addresses
duplicated_mask <- duplicated(merged_addresses[, .SD, .SDcols = address_columns]) |
duplicated(merged_addresses[, .SD, .SDcols = address_columns], fromLast = TRUE)
duplicates <- merged_addresses[duplicated_mask, ]
merged_addresses$recipient_id <- replicate(nrow(merged_addresses), paste(sample(LETTERS, 6, replace=TRUE), collapse=""))
duplicated_mask_ids <- duplicated(as.data.frame(merged_addresses)["recipient_id"]) |
duplicated(as.data.frame(merged_addresses)["recipient_id"], fromLast = TRUE)
duplicates_ids <- merged_addresses[duplicated_mask_ids, ]
# Remove duplicates, keeping the first occurrence
unique_addresses <- merged_addresses %>%
distinct(across(all_of(address_columns)), .keep_all = TRUE)
# Manually remove any unwanted addresses using recipient_id:
unique_addresses <- unique_addresses %>%
# Remove rows where 'unit' contains 'nan' (case-insensitive)
filter(!grepl("nan", unit, ignore.case = TRUE)) %>%
arrange(state, city)
unique_addresses <- unique_addresses %>%
# Rename 'recipeeint id' to 'Participant ID'
rename(`Participant ID` = `recipient_id`)
# Create a frequency table of addresses per state
address_counts <- table(unique_addresses$state)
# Convert the table to a data frame for better readability
address_counts_df <- as.data.frame(address_counts)
# Rename the columns for clarity
colnames(address_counts_df) <- c("State", "Number_of_Addresses")
# Define the final output file path
final_path <- "/Users/amirgazar/Documents/GitHub/WTP Survey/openaddresses datasets/Final/All Addresses/all_addresses.csv"
View(address_counts_df)
View(unique_addresses)
# Load necessary library
library(data.table)
# Define paths and parameters
original_R_file <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 ARC Codes/2 Dispatch Curve.R" # Full path to the original R file
output_folder <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 RCodes to submit/1 Comp Days" # Full path to the folder where new files will be saved
n_sim <- 40  # Number of iterations
# Loop to generate R and Bash files for each pathway and iteration,
# with 'i' taking values 10, 20, ..., 200.
for (i in seq(10, n_sim * 10, by = 10)) {
# ----- Modify R File -----
# Read the original R file
content <- readLines(original_R_file)
# Modify the variable for iteration and pathway_id
content <- gsub(
pattern = paste0("sim_start <- .*"),
replacement = paste0("sim_start <- ", i),
x = content
)
# Save the modified R file with pathway-specific and iteration-specific naming
new_r_file <- file.path(output_folder, paste0("Dispatch_Curve", "_iteration_", i, ".R"))
writeLines(content, new_r_file)
# ----- Generate Bash Script -----
# Define bash script content
bash_script <- paste0("#!/bin/bash
#SBATCH --account=epadecarb
#SBATCH --partition=normal_q
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=80
#SBATCH --cpus-per-task=1  # Reduce the number of cores to avoid OOM
#SBATCH --time=24:00:00    # Set a more appropriate time limit
module load containers/singularity
singularity exec --bind /projects /projects/arcsingularity/ood-rstudio141717-basic_4.1.0.sif Rscript \"/projects/epadecarb/2 Generation Expansion Model/2 R Codes/1 Comp Days/Dispatch_Curve", "_iteration_", i, ".R\"
module reset")
# Save the bash script with pathway-specific and iteration-specific naming
bash_file <- file.path(output_folder, paste0("Dispatch_Curve", "_iteration_", i, ".sh"))
writeLines(bash_script, bash_file)
}
# Install/Import SSH library
library(ssh)
setwd("/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing")
host <- "tinkercliffs2.arc.vt.edu"
username <- "amirgazar" # VT PID
password <- "V?i7Bj!-a3AEZHP"  # VT.edu Password
session <- ssh::ssh_connect(paste0(username, "@", host), passwd = password) # Make sure you authenticate DUO Security
# Uploading Rcode
Rcode_folder <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 RCodes to submit/1 Comp Days" # Path to local folder containing R files
remote_folder <- "/projects/epadecarb/2 Generation Expansion Model/2 R Codes/1 Comp Days/" # Remote folder path
r_files <- list.files(Rcode_folder, pattern = "\\.R$", full.names = TRUE)
for (r_file in r_files) {
ssh::scp_upload(session, r_file, to = remote_folder)
}
# Load bash script
# Get list of Bash files in the folder
bash_files <- list.files(Rcode_folder, pattern = "\\.sh$", full.names = TRUE)
# Loop through each Bash file
for (bash_file in bash_files) {
# Get the base name of the Bash file
bash_file_name <- basename(bash_file)
ssh::scp_upload(session, bash_file, to = remote_folder)
# Execute the uploaded Bash script on the remote server
commands <- paste0('cd "', remote_folder, '" && sbatch -A epadecarb ', bash_file_name)
ssh::ssh_exec_wait(session, command = commands)
}
# Load necessary library
library(data.table)
# Define paths and parameters
original_R_file <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 ARC Codes/2 Dispatch Curve.R" # Full path to the original R file
output_folder <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 RCodes to submit/1 Comp Days" # Full path to the folder where new files will be saved
n_sim <- 40  # Number of iterations
# Loop to generate R and Bash files for each pathway and iteration,
# with 'i' taking values 10, 20, ..., 200.
for (i in seq(10, n_sim * 10, by = 10)) {
# ----- Modify R File -----
# Read the original R file
content <- readLines(original_R_file)
# Modify the variable for iteration and pathway_id
content <- gsub(
pattern = paste0("sim_start <- .*"),
replacement = paste0("sim_start <- ", i),
x = content
)
# Save the modified R file with pathway-specific and iteration-specific naming
new_r_file <- file.path(output_folder, paste0("Dispatch_Curve", "_iteration_", i, ".R"))
writeLines(content, new_r_file)
# ----- Generate Bash Script -----
# Define bash script content
bash_script <- paste0("#!/bin/bash
#SBATCH --account=epadecarb
#SBATCH --partition=normal_q
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=80
#SBATCH --cpus-per-task=1  # Reduce the number of cores to avoid OOM
#SBATCH --time=24:00:00    # Set a more appropriate time limit
module load containers/singularity
singularity exec --bind /projects /projects/arcsingularity/ood-rstudio141717-basic_4.1.0.sif Rscript \"/projects/epadecarb/2 Generation Expansion Model/2 R Codes/1 Comp Days/Dispatch_Curve", "_iteration_", i, ".R\"
module reset")
# Save the bash script with pathway-specific and iteration-specific naming
bash_file <- file.path(output_folder, paste0("Dispatch_Curve", "_iteration_", i, ".sh"))
writeLines(bash_script, bash_file)
}
# Uploading Rcode
Rcode_folder <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 RCodes to submit/1 Comp Days" # Path to local folder containing R files
remote_folder <- "/projects/epadecarb/2 Generation Expansion Model/2 R Codes/1 Comp Days/" # Remote folder path
r_files <- list.files(Rcode_folder, pattern = "\\.R$", full.names = TRUE)
for (r_file in r_files) {
ssh::scp_upload(session, r_file, to = remote_folder)
}
# Uploading Rcode
Rcode_folder <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 RCodes to submit/1 Comp Days" # Path to local folder containing R files
remote_folder <- "/projects/epadecarb/2 Generation Expansion Model/2 R Codes/1 Comp Days/" # Remote folder path
r_files <- list.files(Rcode_folder, pattern = "\\.R$", full.names = TRUE)
for (r_file in r_files) {
ssh::scp_upload(session, r_file, to = remote_folder)
}
# Load bash script
# Get list of Bash files in the folder
bash_files <- list.files(Rcode_folder, pattern = "\\.sh$", full.names = TRUE)
# Loop through each Bash file
for (bash_file in bash_files) {
# Get the base name of the Bash file
bash_file_name <- basename(bash_file)
ssh::scp_upload(session, bash_file, to = remote_folder)
# Execute the uploaded Bash script on the remote server
commands <- paste0('cd "', remote_folder, '" && sbatch -A epadecarb ', bash_file_name)
ssh::ssh_exec_wait(session, command = commands)
}
# Disconnect
#-----
ssh::ssh_disconnect(session)
# Load necessary library
library(data.table)
# Define paths and parameters
original_R_file <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 ARC Codes/2 Dispatch Curve.R" # Full path to the original R file
output_folder <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 RCodes to submit/1 Comp Days" # Full path to the folder where new files will be saved
n_sim <- 80  # Number of iterations
# Loop to generate R and Bash files for each pathway and iteration,
# with 'i' taking values 10, 20, ..., 200.
for (i in seq(10, n_sim * 10, by = 10)) {
# ----- Modify R File -----
# Read the original R file
content <- readLines(original_R_file)
# Modify the variable for iteration and pathway_id
content <- gsub(
pattern = paste0("sim_start <- .*"),
replacement = paste0("sim_start <- ", i),
x = content
)
# Save the modified R file with pathway-specific and iteration-specific naming
new_r_file <- file.path(output_folder, paste0("Dispatch_Curve", "_iteration_", i, ".R"))
writeLines(content, new_r_file)
# ----- Generate Bash Script -----
# Define bash script content
bash_script <- paste0("#!/bin/bash
#SBATCH --account=epadecarb
#SBATCH --partition=normal_q
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=80
#SBATCH --cpus-per-task=1  # Reduce the number of cores to avoid OOM
#SBATCH --time=24:00:00    # Set a more appropriate time limit
module load containers/singularity
singularity exec --bind /projects /projects/arcsingularity/ood-rstudio141717-basic_4.1.0.sif Rscript \"/projects/epadecarb/2 Generation Expansion Model/2 R Codes/1 Comp Days/Dispatch_Curve", "_iteration_", i, ".R\"
module reset")
# Save the bash script with pathway-specific and iteration-specific naming
bash_file <- file.path(output_folder, paste0("Dispatch_Curve", "_iteration_", i, ".sh"))
writeLines(bash_script, bash_file)
}
# Install/Import SSH library
library(ssh)
setwd("/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing")
host <- "tinkercliffs2.arc.vt.edu"
username <- "amirgazar" # VT PID
password <- "V?i7Bj!-a3AEZHP"  # VT.edu Password
session <- ssh::ssh_connect(paste0(username, "@", host), passwd = password) # Make sure you authenticate DUO Security
#---- Batch processing
# Uploading Rcode
Rcode_folder <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 RCodes to submit/1 Comp Days" # Path to local folder containing R files
remote_folder <- "/projects/epadecarb/2 Generation Expansion Model/2 R Codes/1 Comp Days/" # Remote folder path
r_files <- list.files(Rcode_folder, pattern = "\\.R$", full.names = TRUE)
for (r_file in r_files) {
ssh::scp_upload(session, r_file, to = remote_folder)
}
# Load bash script
# Get list of Bash files in the folder
bash_files <- list.files(Rcode_folder, pattern = "\\.sh$", full.names = TRUE)
# Loop through each Bash file
for (bash_file in bash_files) {
# Get the base name of the Bash file
bash_file_name <- basename(bash_file)
ssh::scp_upload(session, bash_file, to = remote_folder)
# Execute the uploaded Bash script on the remote server
commands <- paste0('cd "', remote_folder, '" && sbatch -A epadecarb ', bash_file_name)
ssh::ssh_exec_wait(session, command = commands)
}
# Disconnect
#-----
ssh::ssh_disconnect(session)
# Load necessary library
library(data.table)
# Define paths and parameters
original_R_file <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 ARC Codes/2 Dispatch Curve.R" # Full path to the original R file
output_folder <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 RCodes to submit/1 Comp Days" # Full path to the folder where new files will be saved
n_sim <- 120  # Number of iterations
# Loop to generate R and Bash files for each pathway and iteration,
# with 'i' taking values 10, 20, ..., 200.
for (i in seq(10, n_sim * 10, by = 10)) {
# ----- Modify R File -----
# Read the original R file
content <- readLines(original_R_file)
# Modify the variable for iteration and pathway_id
content <- gsub(
pattern = paste0("sim_start <- .*"),
replacement = paste0("sim_start <- ", i),
x = content
)
# Save the modified R file with pathway-specific and iteration-specific naming
new_r_file <- file.path(output_folder, paste0("Dispatch_Curve", "_iteration_", i, ".R"))
writeLines(content, new_r_file)
# ----- Generate Bash Script -----
# Define bash script content
bash_script <- paste0("#!/bin/bash
#SBATCH --account=epadecarb
#SBATCH --partition=normal_q
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=80
#SBATCH --cpus-per-task=1  # Reduce the number of cores to avoid OOM
#SBATCH --time=24:00:00    # Set a more appropriate time limit
module load containers/singularity
singularity exec --bind /projects /projects/arcsingularity/ood-rstudio141717-basic_4.1.0.sif Rscript \"/projects/epadecarb/2 Generation Expansion Model/2 R Codes/1 Comp Days/Dispatch_Curve", "_iteration_", i, ".R\"
module reset")
# Save the bash script with pathway-specific and iteration-specific naming
bash_file <- file.path(output_folder, paste0("Dispatch_Curve", "_iteration_", i, ".sh"))
writeLines(bash_script, bash_file)
}
# Install/Import SSH library
library(ssh)
setwd("/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing")
host <- "tinkercliffs2.arc.vt.edu"
username <- "amirgazar" # VT PID
password <- "V?i7Bj!-a3AEZHP"  # VT.edu Password
session <- ssh::ssh_connect(paste0(username, "@", host), passwd = password) # Make sure you authenticate DUO Security
#---- Batch processing
# Uploading Rcode
Rcode_folder <- "/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing/1 RCodes to submit/1 Comp Days" # Path to local folder containing R files
remote_folder <- "/projects/epadecarb/2 Generation Expansion Model/2 R Codes/1 Comp Days/" # Remote folder path
r_files <- list.files(Rcode_folder, pattern = "\\.R$", full.names = TRUE)
for (r_file in r_files) {
ssh::scp_upload(session, r_file, to = remote_folder)
}
# Load bash script
# Get list of Bash files in the folder
bash_files <- list.files(Rcode_folder, pattern = "\\.sh$", full.names = TRUE)
# Loop through each Bash file
for (bash_file in bash_files) {
# Get the base name of the Bash file
bash_file_name <- basename(bash_file)
ssh::scp_upload(session, bash_file, to = remote_folder)
# Execute the uploaded Bash script on the remote server
commands <- paste0('cd "', remote_folder, '" && sbatch -A epadecarb ', bash_file_name)
ssh::ssh_exec_wait(session, command = commands)
}
# Disconnect
#-----
ssh::ssh_disconnect(session)
