total_CO_mUSD      = mean(total_CO_USD, na.rm = TRUE)/1e6,
total_AP_mUSD = sum(total_PM2.5_mUSD, total_PM10_mUSD, total_NOx_mUSD, total_SO2_mUSD,
total_VOC_mUSD, total_CO_mUSD),
AP_death_per_year = total_AP_mUSD/mortality_million_USD_2024,
.groups = "drop"
)
death_data <- mean_simulations %>%
select(Facility_Unit.ID, Year, AP_death_per_year)
View(death_data)
death_data <- mean_simulations %>%
select(Facility_Unit.ID, Year, AP_death_per_year) %>%
group_by(Facility_Unit.ID, Year) %>%
summarise(total_death = sum(AP_death_per_year, na.rm = TRUE), .groups = "drop")
View(death_data)
death_data <- mean_simulations %>%
select(Facility_Unit.ID, Year, AP_death_per_year) %>%
group_by(Facility_Unit.ID) %>%
summarise(total_death = sum(AP_death_per_year, na.rm = TRUE), .groups = "drop")
View(death_data)
# Merge with subset_data, adding only the death column
merged_data <- subset_data %>%
inner_join(death_data, by = "Facility_Unit.ID")
merged_data$total_AP_death <- merged_data$AP_death_per_year * 50
# Merge with subset_data, adding only the death column
merged_data <- subset_data %>%
inner_join(death_data, by = "Facility_Unit.ID")
# adjust the last row (to ensure the exact amount of cumsum is achieved)
factor <- (merged_data$cum_mean[merged_data$Facility_Unit.ID == "6156_NHB1"] - new_capacity) /
merged_data$mean_gen_MW[merged_data$Facility_Unit.ID == "6156_NHB1"]
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] <-
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] * factor
death_data <- mean_simulations %>%
select(Facility_Unit.ID, Year, AP_death_per_year) %>%
group_by(Facility_Unit.ID) %>%
summarise(total_AP_death = sum(AP_death_per_year, na.rm = TRUE), .groups = "drop")
# Merge with subset_data, adding only the death column
merged_data <- subset_data %>%
inner_join(death_data, by = "Facility_Unit.ID")
# adjust the last row (to ensure the exact amount of cumsum is achieved)
factor <- (merged_data$cum_mean[merged_data$Facility_Unit.ID == "6156_NHB1"] - new_capacity) /
merged_data$mean_gen_MW[merged_data$Facility_Unit.ID == "6156_NHB1"]
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] <-
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] * factor
View(merged_data)
View(merged_data)
final_data <- merged_data %>%
select(State, total_AP_death) %>%
group_by(State) %>%
summarise(total_AP_death_state = sum(total_AP_death, na.rm = TRUE), .groups = "drop")
View(final_data)
sum(final_data$total_AP_death_state)
4.12163043/9.105593
# Dispatch curve, offsetting demand with generation
library(data.table)
library(httr)
library(htmltools)
library(jsonlite)
library(lubridate)
calculate_npv <- function(dt, rate, base_year, col) {
npv <- sum(dt[[col]] / (1 + rate)^(dt[["Year"]] - base_year))
return(npv)
}
discount_rate <- 0.03
base_year <- 2024
# Cost of mortality: $11.4 million in 2024-USD per avoided mortality
mortality_million_USD_2024 <- 11.4
# 1.1 Fossil Fuels Data
file_path <- "/Users/amirgazar/Documents/GitHub/EPA_Debarbonization/2 Generation Expansion Model/2 Generation/2 Fossil Generation/1 Existing Fossil Fuels/1 Fossil Fuels Facilities Data/Fossil_Fuel_Facilities_Data.csv"
Fossil_Fuels_NPC <- fread(file_path)
Fossil_Fuels_NPC <- Fossil_Fuels_NPC[Retirement_year >= 2025]
# Order data by CF (smallest to largest)
Fossil_Fuels_NPC_sorted <- Fossil_Fuels_NPC[order(Fossil_Fuels_NPC$CF), ]
# Compute cumulative sum of mean_gen_MW
Fossil_Fuels_NPC_sorted$cum_mean <- cumsum(Fossil_Fuels_NPC_sorted$mean_gen_MW)
# Find the index where the cumulative sum reaches or exceeds the new project's capacity
new_capacity <- 900 # Transmission scenario
new_capacity <- 541.63032 # Offshore wind scenario
idx <- which(Fossil_Fuels_NPC_sorted$cum_mean >= new_capacity)[1]
# Subset the rows that contribute to the cumulative sum
subset_data <- Fossil_Fuels_NPC_sorted[1:idx, ]
# RUN 10_Air_emissions.R CODE FIRST to generate Facility_Level_Results
filtered_data <- Facility_Level_Results %>%
filter(Pathway == "A",
Facility_Unit.ID %in% subset_data$Facility_Unit.ID)
mean_simulations <- filtered_data %>%
group_by(Facility_Unit.ID, Year) %>%
summarise(
Year                = first(Year),         # constant across sims
total_generation_GWh= mean(total_generation_GWh, na.rm = TRUE),
total_PM2.5_mUSD      = mean(total_PM2.5_USD, na.rm = TRUE)/1e6,
total_PM10_mUSD      = mean(total_PM10_USD, na.rm = TRUE)/1e6,
total_NOx_mUSD       = mean(total_NOx_USD, na.rm = TRUE)/1e6,
total_SO2_mUSD       = mean(total_SO2_USD, na.rm = TRUE)/1e6,
total_VOC_mUSD      = mean(total_VOC_USD, na.rm = TRUE)/1e6,
total_CO_mUSD      = mean(total_CO_USD, na.rm = TRUE)/1e6,
total_AP_mUSD = sum(total_PM2.5_mUSD, total_PM10_mUSD, total_NOx_mUSD, total_SO2_mUSD,
total_VOC_mUSD, total_CO_mUSD),
AP_death_per_year = total_AP_mUSD/mortality_million_USD_2024,
.groups = "drop"
)
death_data <- mean_simulations %>%
select(Facility_Unit.ID, Year, AP_death_per_year) %>%
group_by(Facility_Unit.ID) %>%
summarise(total_AP_death = sum(AP_death_per_year, na.rm = TRUE), .groups = "drop")
# Merge with subset_data, adding only the death column
merged_data <- subset_data %>%
inner_join(death_data, by = "Facility_Unit.ID")
# adjust the last row (to ensure the exact amount of cumsum is achieved)
factor <- (merged_data$cum_mean[merged_data$Facility_Unit.ID == "6156_NHB1"] - new_capacity) /
merged_data$mean_gen_MW[merged_data$Facility_Unit.ID == "6156_NHB1"]
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] <-
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] * factor
final_data <- merged_data %>%
select(State, total_AP_death) %>%
group_by(State) %>%
summarise(total_AP_death_state = sum(total_AP_death, na.rm = TRUE), .groups = "drop")
View(final_data)
sum(final_data$total_AP_death_state)
# Dispatch curve, offsetting demand with generation
library(data.table)
library(httr)
library(htmltools)
library(jsonlite)
library(lubridate)
calculate_npv <- function(dt, rate, base_year, col) {
npv <- sum(dt[[col]] / (1 + rate)^(dt[["Year"]] - base_year))
return(npv)
}
discount_rate <- 0.03
base_year <- 2024
# Cost of mortality: $11.4 million in 2024-USD per avoided mortality
mortality_million_USD_2024 <- 11.4
# 1.1 Fossil Fuels Data
file_path <- "/Users/amirgazar/Documents/GitHub/EPA_Debarbonization/2 Generation Expansion Model/2 Generation/2 Fossil Generation/1 Existing Fossil Fuels/1 Fossil Fuels Facilities Data/Fossil_Fuel_Facilities_Data.csv"
Fossil_Fuels_NPC <- fread(file_path)
Fossil_Fuels_NPC <- Fossil_Fuels_NPC[Retirement_year >= 2025]
# Order data by CF (smallest to largest)
Fossil_Fuels_NPC_sorted <- Fossil_Fuels_NPC[order(Fossil_Fuels_NPC$CF), ]
# Compute cumulative sum of mean_gen_MW
Fossil_Fuels_NPC_sorted$cum_mean <- cumsum(Fossil_Fuels_NPC_sorted$mean_gen_MW)
# Find the index where the cumulative sum reaches or exceeds the new project's capacity
new_capacity <- 900 # Transmission scenario
#new_capacity <- 541.63032 # Offshore wind scenario
idx <- which(Fossil_Fuels_NPC_sorted$cum_mean >= new_capacity)[1]
# Subset the rows that contribute to the cumulative sum
subset_data <- Fossil_Fuels_NPC_sorted[1:idx, ]
# RUN 10_Air_emissions.R CODE FIRST to generate Facility_Level_Results
filtered_data <- Facility_Level_Results %>%
filter(Pathway == "A",
Facility_Unit.ID %in% subset_data$Facility_Unit.ID)
mean_simulations <- filtered_data %>%
group_by(Facility_Unit.ID, Year) %>%
summarise(
Year                = first(Year),         # constant across sims
total_generation_GWh= mean(total_generation_GWh, na.rm = TRUE),
total_PM2.5_mUSD      = mean(total_PM2.5_USD, na.rm = TRUE)/1e6,
total_PM10_mUSD      = mean(total_PM10_USD, na.rm = TRUE)/1e6,
total_NOx_mUSD       = mean(total_NOx_USD, na.rm = TRUE)/1e6,
total_SO2_mUSD       = mean(total_SO2_USD, na.rm = TRUE)/1e6,
total_VOC_mUSD      = mean(total_VOC_USD, na.rm = TRUE)/1e6,
total_CO_mUSD      = mean(total_CO_USD, na.rm = TRUE)/1e6,
total_AP_mUSD = sum(total_PM2.5_mUSD, total_PM10_mUSD, total_NOx_mUSD, total_SO2_mUSD,
total_VOC_mUSD, total_CO_mUSD),
AP_death_per_year = total_AP_mUSD/mortality_million_USD_2024,
.groups = "drop"
)
death_data <- mean_simulations %>%
select(Facility_Unit.ID, Year, AP_death_per_year) %>%
group_by(Facility_Unit.ID) %>%
summarise(total_AP_death = sum(AP_death_per_year, na.rm = TRUE), .groups = "drop")
# Merge with subset_data, adding only the death column
merged_data <- subset_data %>%
inner_join(death_data, by = "Facility_Unit.ID")
# adjust the last row (to ensure the exact amount of cumsum is achieved)
factor <- (merged_data$cum_mean[merged_data$Facility_Unit.ID == "6156_NHB1"] - new_capacity) /
merged_data$mean_gen_MW[merged_data$Facility_Unit.ID == "6156_NHB1"]
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] <-
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] * factor
final_data <- merged_data %>%
select(State, total_AP_death) %>%
group_by(State) %>%
summarise(total_AP_death_state = sum(total_AP_death, na.rm = TRUE), .groups = "drop")
View(subset_data)
View(merged_data)
# Facility Capacity Data
facility_data <- subset_data %>%
select(Unit_Type, Estimated_NameplateCapacity_MW) %>%
group_by(Unit_Type) %>%
summarise(total_NPC = sum(Estimated_NameplateCapacity_MW, na.rm = TRUE), .groups = "drop")
View(facility_data)
# Facility Capacity Data
subset_data$Estimated_NameplateCapacity_MW[subset_data$Facility_Unit.ID == "6156_NHB1"] <-
subset_data$Estimated_NameplateCapacity_MW[subset_data$Facility_Unit.ID == "6156_NHB1"] * factor
facility_data <- subset_data %>%
select(Unit_Type, Estimated_NameplateCapacity_MW) %>%
group_by(Unit_Type) %>%
summarise(total_NPC = sum(Estimated_NameplateCapacity_MW, na.rm = TRUE), .groups = "drop")
View(facility_data)
1499.20+
270.00
1769.2+1043.04
2812.24*0.6
2812.24*0.2
900/2812.24
View(subset_data)
subset_data$CF
subset_data$Real_NameplateCapacity_MW <- subset_data$Estimated_NameplateCapacity_MW * subset_data$CF
View(subset_data)
# Dispatch curve, offsetting demand with generation
library(data.table)
library(httr)
library(htmltools)
library(jsonlite)
library(lubridate)
calculate_npv <- function(dt, rate, base_year, col) {
npv <- sum(dt[[col]] / (1 + rate)^(dt[["Year"]] - base_year))
return(npv)
}
discount_rate <- 0.03
base_year <- 2024
# Cost of mortality: $11.4 million in 2024-USD per avoided mortality
mortality_million_USD_2024 <- 11.4
# 1.1 Fossil Fuels Data
file_path <- "/Users/amirgazar/Documents/GitHub/EPA_Debarbonization/2 Generation Expansion Model/2 Generation/2 Fossil Generation/1 Existing Fossil Fuels/1 Fossil Fuels Facilities Data/Fossil_Fuel_Facilities_Data.csv"
Fossil_Fuels_NPC <- fread(file_path)
Fossil_Fuels_NPC <- Fossil_Fuels_NPC[Retirement_year >= 2025]
# Order data by CF (smallest to largest)
Fossil_Fuels_NPC_sorted <- Fossil_Fuels_NPC[order(Fossil_Fuels_NPC$CF), ]
# Compute cumulative sum of mean_gen_MW
Fossil_Fuels_NPC_sorted$cum_mean <- cumsum(Fossil_Fuels_NPC_sorted$mean_gen_MW)
# Find the index where the cumulative sum reaches or exceeds the new project's capacity
new_capacity <- 900 # Transmission scenario
#new_capacity <- 541.63032 # Offshore wind scenario
idx <- which(Fossil_Fuels_NPC_sorted$cum_mean >= new_capacity)[1]
# Subset the rows that contribute to the cumulative sum
subset_data <- Fossil_Fuels_NPC_sorted[1:idx, ]
# RUN 10_Air_emissions.R CODE FIRST to generate Facility_Level_Results
filtered_data <- Facility_Level_Results %>%
filter(Pathway == "A",
Facility_Unit.ID %in% subset_data$Facility_Unit.ID)
mean_simulations <- filtered_data %>%
group_by(Facility_Unit.ID, Year) %>%
summarise(
Year                = first(Year),         # constant across sims
total_generation_GWh= mean(total_generation_GWh, na.rm = TRUE),
total_PM2.5_mUSD      = mean(total_PM2.5_USD, na.rm = TRUE)/1e6,
total_PM10_mUSD      = mean(total_PM10_USD, na.rm = TRUE)/1e6,
total_NOx_mUSD       = mean(total_NOx_USD, na.rm = TRUE)/1e6,
total_SO2_mUSD       = mean(total_SO2_USD, na.rm = TRUE)/1e6,
total_VOC_mUSD      = mean(total_VOC_USD, na.rm = TRUE)/1e6,
total_CO_mUSD      = mean(total_CO_USD, na.rm = TRUE)/1e6,
total_AP_mUSD = sum(total_PM2.5_mUSD, total_PM10_mUSD, total_NOx_mUSD, total_SO2_mUSD,
total_VOC_mUSD, total_CO_mUSD),
AP_death_per_year = total_AP_mUSD/mortality_million_USD_2024,
.groups = "drop"
)
death_data <- mean_simulations %>%
select(Facility_Unit.ID, Year, AP_death_per_year) %>%
group_by(Facility_Unit.ID) %>%
summarise(total_AP_death = sum(AP_death_per_year, na.rm = TRUE), .groups = "drop")
# Merge with subset_data, adding only the death column
merged_data <- subset_data %>%
inner_join(death_data, by = "Facility_Unit.ID")
# adjust the last row (to ensure the exact amount of cumsum is achieved)
factor <- (merged_data$cum_mean[merged_data$Facility_Unit.ID == "6156_NHB1"] - new_capacity) /
merged_data$mean_gen_MW[merged_data$Facility_Unit.ID == "6156_NHB1"]
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] <-
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] * factor
final_data <- merged_data %>%
select(State, total_AP_death) %>%
group_by(State) %>%
summarise(total_AP_death_state = sum(total_AP_death, na.rm = TRUE), .groups = "drop")
# Facility Capacity Data
subset_data$Estimated_NameplateCapacity_MW[subset_data$Facility_Unit.ID == "6156_NHB1"] <-
subset_data$Estimated_NameplateCapacity_MW[subset_data$Facility_Unit.ID == "6156_NHB1"] * factor
subset_data$Real_NameplateCapacity_MW <- subset_data$Estimated_NameplateCapacity_MW * subset_data$CF
facility_data <- subset_data %>%
select(Unit_Type, Real_NameplateCapacity_MW) %>%
group_by(Unit_Type) %>%
summarise(total_NPC = sum(Real_NameplateCapacity_MW, na.rm = TRUE), .groups = "drop")
View(facility_data)
463.28426+70.37462
# Dispatch curve, offsetting demand with generation
library(data.table)
library(httr)
library(htmltools)
library(jsonlite)
library(lubridate)
calculate_npv <- function(dt, rate, base_year, col) {
npv <- sum(dt[[col]] / (1 + rate)^(dt[["Year"]] - base_year))
return(npv)
}
discount_rate <- 0.03
base_year <- 2024
# Cost of mortality: $11.4 million in 2024-USD per avoided mortality
mortality_million_USD_2024 <- 11.4
# 1.1 Fossil Fuels Data
file_path <- "/Users/amirgazar/Documents/GitHub/EPA_Debarbonization/2 Generation Expansion Model/2 Generation/2 Fossil Generation/1 Existing Fossil Fuels/1 Fossil Fuels Facilities Data/Fossil_Fuel_Facilities_Data.csv"
Fossil_Fuels_NPC <- fread(file_path)
Fossil_Fuels_NPC <- Fossil_Fuels_NPC[Retirement_year >= 2025]
# Order data by CF (smallest to largest)
Fossil_Fuels_NPC_sorted <- Fossil_Fuels_NPC[order(Fossil_Fuels_NPC$CF), ]
# Compute cumulative sum of mean_gen_MW
Fossil_Fuels_NPC_sorted$cum_mean <- cumsum(Fossil_Fuels_NPC_sorted$mean_gen_MW)
# Find the index where the cumulative sum reaches or exceeds the new project's capacity
new_capacity <- 900 # Transmission scenario
new_capacity <- 541.63032 # Offshore wind scenario
idx <- which(Fossil_Fuels_NPC_sorted$cum_mean >= new_capacity)[1]
# Subset the rows that contribute to the cumulative sum
subset_data <- Fossil_Fuels_NPC_sorted[1:idx, ]
# RUN 10_Air_emissions.R CODE FIRST to generate Facility_Level_Results
filtered_data <- Facility_Level_Results %>%
filter(Pathway == "A",
Facility_Unit.ID %in% subset_data$Facility_Unit.ID)
mean_simulations <- filtered_data %>%
group_by(Facility_Unit.ID, Year) %>%
summarise(
Year                = first(Year),         # constant across sims
total_generation_GWh= mean(total_generation_GWh, na.rm = TRUE),
total_PM2.5_mUSD      = mean(total_PM2.5_USD, na.rm = TRUE)/1e6,
total_PM10_mUSD      = mean(total_PM10_USD, na.rm = TRUE)/1e6,
total_NOx_mUSD       = mean(total_NOx_USD, na.rm = TRUE)/1e6,
total_SO2_mUSD       = mean(total_SO2_USD, na.rm = TRUE)/1e6,
total_VOC_mUSD      = mean(total_VOC_USD, na.rm = TRUE)/1e6,
total_CO_mUSD      = mean(total_CO_USD, na.rm = TRUE)/1e6,
total_AP_mUSD = sum(total_PM2.5_mUSD, total_PM10_mUSD, total_NOx_mUSD, total_SO2_mUSD,
total_VOC_mUSD, total_CO_mUSD),
AP_death_per_year = total_AP_mUSD/mortality_million_USD_2024,
.groups = "drop"
)
death_data <- mean_simulations %>%
select(Facility_Unit.ID, Year, AP_death_per_year) %>%
group_by(Facility_Unit.ID) %>%
summarise(total_AP_death = sum(AP_death_per_year, na.rm = TRUE), .groups = "drop")
# Merge with subset_data, adding only the death column
merged_data <- subset_data %>%
inner_join(death_data, by = "Facility_Unit.ID")
# adjust the last row (to ensure the exact amount of cumsum is achieved)
factor <- (merged_data$cum_mean[merged_data$Facility_Unit.ID == "6156_NHB1"] - new_capacity) /
merged_data$mean_gen_MW[merged_data$Facility_Unit.ID == "6156_NHB1"]
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] <-
merged_data$total_AP_death[merged_data$Facility_Unit.ID == "6156_NHB1"] * factor
final_data <- merged_data %>%
select(State, total_AP_death) %>%
group_by(State) %>%
summarise(total_AP_death_state = sum(total_AP_death, na.rm = TRUE), .groups = "drop")
# Facility Capacity Data
subset_data$Estimated_NameplateCapacity_MW[subset_data$Facility_Unit.ID == "6156_NHB1"] <-
subset_data$Estimated_NameplateCapacity_MW[subset_data$Facility_Unit.ID == "6156_NHB1"] * factor
subset_data$Real_NameplateCapacity_MW <- subset_data$Estimated_NameplateCapacity_MW * subset_data$CF
facility_data <- subset_data %>%
select(Unit_Type, Real_NameplateCapacity_MW) %>%
group_by(Unit_Type) %>%
summarise(total_NPC = sum(Real_NameplateCapacity_MW, na.rm = TRUE), .groups = "drop")
View(facility_data)
399.32747+70.37462
# Load necessary library
library(dplyr)
library(data.table)
# Define state codes and base file path
state_codes <- c("NH", "MA")
base_path <- "/Users/amirgazar/Documents/GitHub/WTP Survey/openaddresses datasets/"
# Initialize an empty list to store each state's address data
address_list <- list()
# Loop through each state and read the corresponding address file
for (state in state_codes) {
# Construct the file path for each state's addresses file
file_path <- paste0(base_path, state, "/random_addresses_", state, ".csv")
# Read the address file and add it to the list if it exists
if (file.exists(file_path)) {
state_data <- read.csv(
file_path,
stringsAsFactors = FALSE,
na.strings = c("NA", "NaN", "NAN")  # Treat "NaN" and "NAN" as NA
)
set.seed(42)  # Optional: Set seed for reproducibility
# Generate the recipient_id based on the rule
state_data <- state_data %>%
mutate(
control_number = sample(0:9, n(), replace = TRUE),  # Random control number (0-9) for each row
control_letter = sample(LETTERS, n(), replace = TRUE),  # Random control letter (A-Z) for each row
recipient_id = paste0(
toupper(substr(address_city, nchar(address_city) - 1, nchar(address_city))), # Last two letters of city
substr(sprintf("%05s", address_ZIPPlus4), 2, 3), # Middle two digits of ZIP code
control_number, control_letter
),
unit = ifelse(is.na(address_secondaryAddress) | address_secondaryAddress == "",
"", address_secondaryAddress),
street = ifelse(is.na(address_streetAddress) | address_streetAddress == "",
"", address_streetAddress),
city = ifelse(is.na(address_city) | address_city == "",
"", address_city),
state = ifelse(is.na(address_state) | address_state == "",
"", address_state),
zip = ifelse(is.na(address_ZIPPlus4) | address_ZIPPlus4 == "",
sprintf("%05s", address_ZIPCode), # Only 5-digit ZIP if ZIP+4 is missing
paste0(sprintf("%05s", address_ZIPCode), "-", address_ZIPPlus4)
)
) %>%
select(recipient_id, unit, street, city, state, zip) %>%
# Replace any remaining NA values with empty strings
mutate(across(everything(), ~ ifelse(is.na(.), "", .)))
# Append to the list
address_list[[state]] <- state_data
} else {
warning(paste("File not found for state:", state))
}
}
# Combine all state data frames into a single data frame
addresses <- bind_rows(address_list)  # Using bind_rows from dplyr for better handling
# ============================================
# Step 1: Identify and Display Duplicate Addresses
# ============================================
# Define the columns that determine a unique address
# Adjust these columns based on your data structure
unique_address_columns <- c("unit", "street", "city", "state", "zip")
# Identify duplicates
duplicates <- addresses %>%
group_by(across(all_of(unique_address_columns))) %>%
filter(n() > 1) %>%
ungroup()
# Check if there are any duplicates
if(nrow(duplicates) > 0){
cat("Duplicate Addresses Found:\n")
print(duplicates)
# Optionally, you can save duplicates to a separate CSV for review
duplicates_path <- "/Users/amirgazar/Documents/GitHub/WTP Survey/openaddresses datasets/Final/duplicate_addresses.csv"
write.csv(duplicates, duplicates_path, row.names = FALSE, na = "")
# ============================================
# Step 2: Remove Duplicate Addresses
# ============================================
# Remove duplicates, keeping the first occurrence
addresses_unique <- addresses %>%
distinct(across(all_of(unique_address_columns)), .keep_all = TRUE)
cat("\nDuplicates have been removed. Number of unique addresses:", nrow(addresses_unique), "\n")
} else {
cat("No duplicate addresses found.\n")
addresses_unique <- addresses
}
# ============================================
# Step 3: Save the Cleaned Data to CSV
# ============================================
# Define the final output file path
final_path <- "/Users/amirgazar/Documents/GitHub/WTP Survey/openaddresses datasets/Final/combined_addresses.csv"
# Save the combined (and deduplicated) addresses to a CSV file in the Final folder
write.csv(addresses_unique, final_path, row.names = FALSE, na = "")  # Write NA as empty strings
cat("Combined addresses have been saved to:", final_path, "\n")
# Define the folder path
folder_path <- "/Users/amirgazar/Documents/GitHub/WTP Survey/openaddresses datasets/Final/"
# Get the list of batch folders
batch_folders <- list.dirs(folder_path, recursive = FALSE)
# Read and merge all batch files dynamically
address_list <- lapply(batch_folders, function(batch) {
file_path <- file.path(batch, "combined_addresses.csv")
if (file.exists(file_path)) {
fread(file_path)
} else {
NULL
}
})
# Remove NULL entries (in case any file is missing)
address_list <- Filter(Negate(is.null), address_list)
# Combine all datasets into one
merged_addresses <- bind_rows(address_list)
address_columns <- c("unit", "street", "city", "state", "zip")
# Create a logical vector indicating duplicated addresses
duplicated_mask <- duplicated(merged_addresses[, .SD, .SDcols = address_columns]) |
duplicated(merged_addresses[, .SD, .SDcols = address_columns], fromLast = TRUE)
duplicates <- merged_addresses[duplicated_mask, ]
merged_addresses$recipient_id <- replicate(nrow(merged_addresses), paste(sample(LETTERS, 6, replace=TRUE), collapse=""))
duplicated_mask_ids <- duplicated(as.data.frame(merged_addresses)["recipient_id"]) |
duplicated(as.data.frame(merged_addresses)["recipient_id"], fromLast = TRUE)
duplicates_ids <- merged_addresses[duplicated_mask_ids, ]
# Remove duplicates, keeping the first occurrence
unique_addresses <- merged_addresses %>%
distinct(across(all_of(address_columns)), .keep_all = TRUE)
# Manually remove any unwanted addresses using recipient_id:
unique_addresses <- unique_addresses %>%
# Remove rows where 'unit' contains 'nan' (case-insensitive)
filter(!grepl("nan", unit, ignore.case = TRUE)) %>%
arrange(state, city)
unique_addresses <- unique_addresses %>%
# Rename 'recipeeint id' to 'Participant ID'
rename(`Participant ID` = `recipient_id`)
# Create a frequency table of addresses per state
address_counts <- table(unique_addresses$state)
# Convert the table to a data frame for better readability
address_counts_df <- as.data.frame(address_counts)
# Rename the columns for clarity
colnames(address_counts_df) <- c("State", "Number_of_Addresses")
# Define the final output file path
final_path <- "/Users/amirgazar/Documents/GitHub/WTP Survey/openaddresses datasets/Final/All Addresses/all_addresses.csv"
View(address_counts_df)
View(unique_addresses)
n_simulations <- 10
sim_start <- 1
sim_idx <- sim_start:(n_simulations + sim_start - 1)
sim_idx
sim_idx <- c(14, 24, 44, 64, 74, 83, 114, 134, 164, 174)
sim_idx
# Install/Import SSH library
library(ssh)
setwd("/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing")
host <- "tinkercliffs2.arc.vt.edu"
username <- "amirgazar" # VT PID
password <- "av@/tNiHBN(LR4("  # VT.edu Password
session <- ssh::ssh_connect(paste0(username, "@", host), passwd = password) # Make sure you authenticate DUO Security
setwd("/Users/amirgazar/Documents/GitHub/Decarbonization-Tradeoffs/2 Generation Expansion Model/5 Dispatch Curve/2 Advanced Research Computing")
host <- "tinkercliffs2.arc.vt.edu"
username <- "amirgazar" # VT PID
password <- "av@/tNiHBN(LR4("  # VT.edu Password
session <- ssh::ssh_connect(paste0(username, "@", host), passwd = password) # Make sure you authenticate DUO Security
